#!/bin/bash

echo "üîã Local LLM Battery Analysis"
echo "============================"

echo "üìä Your Current Setup:"
echo "- Model: Mistral 7B (4.4GB)"
echo "- Hardware: Apple Silicon (M1/M2/M3 likely)"
echo "- Inference: ~19 seconds per categorization"

echo ""
echo "‚ö° Power Consumption Factors:"
echo ""
echo "LOCAL INFERENCE ADVANTAGES:"
echo "‚úÖ No network radio usage (WiFi/4G)"
echo "‚úÖ No API rate limiting delays"
echo "‚úÖ Predictable power usage"
echo "‚úÖ Works offline"
echo ""
echo "LOCAL INFERENCE COSTS:"
echo "‚ùå CPU/GPU intensive computation"
echo "‚ùå Model stays loaded in RAM (4.4GB)"
echo "‚ùå Heat generation from sustained computation"
echo ""
echo "CLOUD API COSTS:"
echo "‚ùå Network transmission power"
echo "‚ùå API service fees ($0.002-0.01/request)"
echo "‚ùå Variable latency (1-5 seconds)"
echo "‚ùå Internet dependency"
echo ""
echo "üéØ BATTERY IMPACT VERDICT:"
echo ""
echo "For LIGHT USAGE (few categorizations/hour):"
echo "ü§î Local ‚âà Cloud (network vs compute trade-off)"
echo ""
echo "For HEAVY USAGE (many categorizations/hour):"
echo "‚ö†Ô∏è  Local WORSE than cloud (sustained CPU load)"
echo ""
echo "For OFFLINE SCENARIOS:"
echo "‚úÖ Local BETTER (no network needed)"
echo ""
echo "üí° OPTIMIZATION STRATEGIES:"
echo ""
echo "1. SMALLER MODELS:"
echo "   ollama pull tinyllama    # 637MB vs 4.4GB"
echo "   ollama pull phi          # 1.6GB, efficient"
echo ""
echo "2. CACHE AGGRESSIVELY:"
echo "   - Your 24h cache TTL is good"
echo "   - Consider 48h for stable activities"
echo ""
echo "3. HYBRID APPROACH:"
echo "   - Local for common categories"
echo "   - Cloud fallback for rare/unknown"
echo ""
echo "4. POWER-AWARE SWITCHING:"
echo "   - Detect battery level"
echo "   - Switch to cloud when <20% battery"